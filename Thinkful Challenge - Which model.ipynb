{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have a fairly substantial starting toolbox of supervised learning methods that you can use to tackle a host of exciting problems. To make sure all of these ideas are organized in your mind, please go through the list of problems below. For each, identify which supervised learning method(s) would be best for addressing that particular problem. Explain your reasoning and discuss your answers with your mentor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Predict the running times of prospective Olympic sprinters using data from the last 20 Olympics.\n",
    "\n",
    "[Runners are slowly getting faster over time, for a variety of reasons](https://www.sportsrec.com/557769-why-were-running-faster-than-ever.html). I would expect that when plotted, these run times would show that steady progress (though I have to imagine the curve is slowly flattening out, as faster times become more difficult to achieve over time...?). \n",
    "\n",
    "What I imagine would be the most important inputs--Olympics year, runner age and weight, maybe bodyfat composition or some other measure of body composition/muscle mass?, VO2 max or some measure of lung capacity--and target (run time) are all continuous. If gender is important, it can easily be encoded. Basic OLS regression is well-suited to the task. \n",
    "\n",
    "It's possible that future run times will be faster than those from previous Olympics--this puts them outside of the ability of a random forest for instance, which can only predict values within a range it has already seen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. You have more features (columns) than rows in your dataset.\n",
    "\n",
    "It's not clear whether the dataset here is small, or whether there are simply more columns than rows (meaning potentially a _lot_ of columns).\n",
    "\n",
    "Ridge or Lasso can help reduce the number/influence of insignificant features. They both work by minimizing the coefficients of features that don't contribute a lot, ideally reducing those coefficients to zero and effectively eliminating the features from consideration.\n",
    "\n",
    "A random forest will not necessarily care about all of the features and may be able to handle this dataset without a lot of problems. \n",
    "\n",
    "Or, do the manual work to reduce the feature set, by generating a correlation matrix and eliminating poorly correlated features, and features that exhibit multicollinearity.\n",
    "\n",
    "I think I probably also need to mention something about how to test with a dataset that is already relatively small...? The easy way to handle this would be to multiply the available rows to make a bigger data set for testing, with the caveat that it could cause the model to place more importance on repeated values. I also remember reading about the SMOTE technique to help with imbalanced data sets--I imagine it could be applied here to generate some good testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Identify the most important characteristic predicting likelihood of being jailed before age 20.\n",
    "\n",
    "A classifier model would be most useful here.\n",
    "\n",
    "A decision tree could be a good candidate:\n",
    "* Being jailed or not is a binary outcome.\n",
    "* I imagine a lot of the data here would be categorical in nature, or could be easily converted to categorical via one-hot encoding, or label encoding.\n",
    "* The ID3 algorithm often used to generate a decision tree would identify the most important features, in order to create rules to split the data on those features first. \n",
    "\n",
    "Logistic regression may also be a good choice here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Implement a filter to “highlight” emails that might be important to the recipient\n",
    "\n",
    "This is a pretty simple, non-critical task. A Naive Bayes classifier can work from a list of keywords or key phrases, with a binary output (either \"important\" or not)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. You have 1000+ features.\n",
    "\n",
    "Again, Ridge or Lasso may help reduce the features.\n",
    "\n",
    "If maintaining the features as-is is not a concern, Principal Component Analysis can combine features with similar variance, allowing you to reduce the set of features to something more manageable and significant.\n",
    "\n",
    "I also chatted with my mentor about this one--he suggested a random forest may be good here. Each individual tree won't necessarily look at all of the features, but together the forest will likely cover them all, and collectively, they will be able to sort through and find the most important features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Predict whether someone who adds items to their cart on a website will purchase the items.\n",
    "\n",
    "This sounds like a classifier with a binary outcome. In the Udemy course material they used a Logit model to perform this task--but we didn't cover that one in the Thinkful material.\n",
    "\n",
    "Of the models we covered, perhaps KNN or SVC? The Support Vector Classifier should allow us to find a hyperplane that divides the two classes (will purchase vs. won't purchase). Similarly, KNN would likely show us that past purchasers have some similar attributes that group them together.\n",
    "\n",
    "A decision tree or random forest could also handle this--maybe a random forest, since the task doesn't specify that we need to know which features the model deems most important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Your dataset dimensions are 982400 x 500\n",
    "\n",
    "Use a model that is amenable to [a partial-fit approach](https://tomaugspurger.github.io/scalable-ml-02.html), if possible.\n",
    "\n",
    "Try Ridge or Lasso, or PCA, to reduce features?\n",
    "\n",
    "This feels kind of similar to \\#5 above, so I'm not sure if there is another tool I should mention...?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Identify faces in an image.\n",
    "\n",
    "This is a task for a classifier. Honestly I don't know much about facial recognition, but I imagine it involves applying some standard patterns to the image to see which are a match, then using the results of those pattern matches to say 1) whether there is a face and 2) where it is in the image.\n",
    "\n",
    "Whether anything else happens after this depends on the definition of \"identify\" here--are we looking for _a_ face? Or by \"identify\", should we understand that we need to decide whose face we found?\n",
    "\n",
    "If we need to figure out whose face we found, that would be another classifier--I imagine some implementation of either a random forest, or a KNN classifier, to use known attributes (and probably a user's friend circle, if this is on any of the social media platforms) to whittle down the possibilities to a best candidate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Predict which of three flavors of ice cream will be most popular with boys vs girls.\n",
    "\n",
    "Ice cream flavors and gender are both categorical, so a classifier is the best fit here. \n",
    "\n",
    "What additional data about the ice cream is available, to help drive the predictions?\n",
    "\n",
    "KNN may be a good choice...? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
